{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1bc760e-14ac-442b-aa40-197eb9ffab5a",
   "metadata": {},
   "source": [
    "# 5G Deployment Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ab145-1745-4286-97bb-94bde0010f86",
   "metadata": {},
   "source": [
    "### Problem Statement:\n",
    "The problem statement and objectives for our project are as follows:\r\n",
    "\r\n",
    "**Problem Statement**:\r\n",
    "The objective of this project is to predict the likelihood of 5G deployment in specific cities or regions based on historical deployment data and other relevant factors.\r\n",
    "\r\n",
    "**Objectives**:\r\n",
    "1. **Predictive Modeling**: Develop machine learning models that can accurately predict the probability or likelihood of 5G deployment in cities or regions.\r\n",
    "2. **Data Analysis**: Analyze historical deployment data and other relevant factors to identify patterns, trends, and correlations that may influence 5G deployment decisions.\r\n",
    "3. **Feature Engineering**: Explore and engineer features that may have predictive power in determining the likelihood of 5G deployment, such as population density, regulatory policies, existing infrastructure, etc.\r\n",
    "4. **Model Interpretability**: Ensure that the developed models are interpretable, allowing stakeholders to understand the factors driving the predictions and make informed decisions based on the model outputs.\r\n",
    "5. **Performance Evaluation**: Evaluate the performance of the developed models using appropriate metrics and techniques, such as accuracy, precision, recall, F1-score, and ROC curves.\r\n",
    "6. **Deployment Recommendations**: Provide actionable insights and recommendations based on the model predictions, highlighting cities or regions with the highest likelihood of 5G deployment and factors contributing to their likelihood.\r\n",
    "7. **Continuous Improvement**: Continuously monitor and refine the models based on feedback and new data to ensure their accuracy and relevance over time.\r\n",
    "8. **Stakeholder Communication**: Effectively communicate the results, insights, and recommendations to stakeholders, including telecommunications companies, policymakers, and regulatory bodies, to inform decision-making and planning related to 5G g.\r\n",
    "\r\n",
    "Specific Goals:\r\n",
    "\r\n",
    "1. Location Prediction: One primary goal is to predict the locations where 5G networks are likely to be deployed. This involves analyzing various factors such as population density, urbanization trends, existing telecommunications infrastructure, regulatory policies, and market demand. The aim is to identify regions or cities with the highest likelihood of 5G deployment, enabling stakeholders to prioritize investment and resources accordingly.\r\n",
    "\r\n",
    "2. Timing Estimation: Another critical goal is to estimate the timing of 5G deployment in different locations. This requires analyzing historical deployment data, technological advancements, regulatory milestones, and market dynamics to forecast when 5G networks are expected to become available in specific regions. Accurate timing estimates can help businesses, policymakers, and consumers prepare for the rollout of 5G services and capitalize on emerging opportunities.\r\n",
    "\r\n",
    "3. Success Rate Assessment: Additionally, predicting the success rate of 5G deployment initiatives is essential for assessing the likelihood of project completion and the attainment of desired outcomes. This involves evaluating factors such as network coverage, reliability, performance, user adoption, and regulatory compliance. By forecasting the success rate of 5G deployment projects, stakeholders can mitigate risks, allocate resources effectively, and optimize project management strategies.\r\n",
    "\r\n",
    "Going Deep:\r\n",
    "\r\n",
    "To address these goals effectively, a comprehensive approach to data science and machine learning is required, encompassing various stages of the project lifecycle:\r\n",
    "\r\n",
    "1. Data Collection and Understanding: Deep analysis and understanding of diverse datasets are essential for capturing the complexities of 5G deployment. This involves collecting data from multiple sources, including telecommunications providers, regulatory agencies, geographic databases, demographic surveys, and industry reports. Understanding the nuances and limitations of each dataset is crucial for extracting actionable insights and ensuring the accuracy and reliability of predictions.\r\n",
    "\r\n",
    "2. Feature Engineering and Selection: Deep feature engineering is necessary to extract meaningful predictors of 5G deployment from raw data. This may involve creating new features, such as proximity to existing infrastructure, socioeconomic indicators, terrain characteristics, and regulatory frameworks. Feature selection techniques, such as correlation analysis, dimensionality reduction, and domain expertise, can help identify the most relevant predictors for inclusion in predictive models.\r\n",
    "\r\n",
    "3. Model Development and Evaluation: Deep learning algorithms, such as neural networks, recurrent neural networks (RNNs), convolutional neural networks (CNNs), and transformers, offer powerful capabilities for predicting complex patterns in 5G deployment data. Advanced machine learning techniques, such as ensemble learning, gradient boosting, and reinforcement learning, can further enhance prediction accuracy and robustness. Model evaluation metrics, such as precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC), provide deep insights into the performance of predictive models and their generalization ability.\r\n",
    "\r\n",
    "4. Interpretability and Explainability: Deep interpretability and explainability techniques are essential for understanding the underlying factors driving 5G deployment predictions. This involves visualizing model predictions, feature importances, and decision boundaries to gain deep insights into the relationships between input variables and output predictions. Explainable AI (XAI) methods, such as feature attribution techniques, SHAP (SHapley Additive exPlanations), and LIME (Local Interpretable Model-agnostic Explanations), facilitate deep understanding and trust in predictive models, enabling stakeholders to make informed decisions based on model outputs.\r\n",
    "\r\n",
    "By embracing a deep and comprehensive approach to problem definition, goal setting, and execution, data scientists and machine learning practitioners can unlock the full potential of predictive analyA dics for 5G deployment. Deep understanding of data, models, and predictions empowers stakeholders to navigate the complexities of 5G deployment and harness the transformative power of emerging telecommunications technologies for the benefit of society.nd investments.ic benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de084e78-2a90-4ce8-9513-f55d41de9615",
   "metadata": {},
   "source": [
    "## Data Understanding:\r\n",
    "Explore the structure of the JSON file to understand how the data is organized. This involves examining the keys, values, and overall hierarchy of the data.\r\n",
    "Identify potential data quality issues, such as missing values, duplicates, or inconsistencies, and decide how to address them during preprocessing.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c444c91-56ea-4cd6-8da6-4d39b55371f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 145917 entries, 0 to 145916\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   operator   145917 non-null  object\n",
      " 1   city_name  145917 non-null  object\n",
      " 2   status     145917 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 3.3+ MB\n",
      "None\n",
      "\n",
      "Cleaned DataFrame:\n",
      "        operator          city_name                   status\n",
      "0        Ooredoo  Abdullah al-Salem  Commercial Availability\n",
      "1          Optus           Canberra  Commercial Availability\n",
      "2          Optus             Sydney  Commercial Availability\n",
      "3  AT&T Mobility   Jacksonville, FL  Commercial Availability\n",
      "4  AT&T Mobility        Atlanta, GA  Commercial Availability\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON file with explicit encoding specification\n",
    "with open('ookla-5g-map.geojson', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract features and properties from the JSON data\n",
    "features = data['features']\n",
    "properties = [feature['properties'] for feature in features]\n",
    "\n",
    "# Convert JSON data to pandas DataFrame\n",
    "df = pd.DataFrame(properties)\n",
    "\n",
    "# Display information about missing values and data types\n",
    "print(\"Data information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Handle missing values\n",
    "# Strategy: Impute missing values for numerical columns with median and categorical columns with mode\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "categorical_cols = df.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "for col in numerical_cols:\n",
    "    median_val = df[col].median()\n",
    "    df[col].fillna(median_val, inplace=True)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    mode_val = df[col].mode()[0]\n",
    "    df[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "# Address outliers and inconsistencies (Optional)\n",
    "# You can perform outlier detection and removal or transformation based on specific requirements.\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(\"\\nCleaned DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Save the cleaned DataFrame to a new JSON file\n",
    "df.to_json('cleaned_ookla_5g_map.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc91d565-ea7d-460d-a5f0-66a0e3a1ae75",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c659b201-7bca-4d5d-bc93-c324f4944e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with outliers and their counts:\n",
      "Series([], dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned DataFrame\n",
    "df = pd.read_json('cleaned_ookla_5g_map.json')\n",
    "\n",
    "# Define a function to detect outliers using Z-score\n",
    "def detect_outliers_zscore(data, threshold=3):\n",
    "    z_scores = (data - data.mean()) / data.std()\n",
    "    return abs(z_scores) > threshold\n",
    "\n",
    "# Detect outliers in numerical columns using Z-score\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "outliers = df[numerical_cols].apply(detect_outliers_zscore)\n",
    "\n",
    "# Count the number of outliers in each numerical column\n",
    "outliers_count = outliers.sum()\n",
    "\n",
    "# Display columns with outliers and their counts\n",
    "print(\"Columns with outliers and their counts:\")\n",
    "print(outliers_count)\n",
    "\n",
    "# Handle outliers by replacing them with median values\n",
    "for col in numerical_cols:\n",
    "    median_val = df[col].median()\n",
    "    df.loc[outliers[col], col] = median_val\n",
    "\n",
    "# Save the DataFrame with outliers handled\n",
    "df.to_json('cleaned_outliers_handled_ookla_5g_map.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61333eef-0ab1-4f63-b084-0abebd31d993",
   "metadata": {},
   "source": [
    "## Convert categorical variables:\n",
    "\n",
    "If the data includes categorical variables, encode them into numerical representations using appropriate techniques\r\n",
    "Normalize or scale numerical features: Bring numerical features to a similar scale to prevent certain features from dominating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3c36ea-5184-4f2b-aa70-6dc02ddb804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "# Load the cleaned DataFrame\n",
    "df = pd.read_json('cleaned_outliers_handled_ookla_5g_map.json')\n",
    "\n",
    "# Convert categorical variables using label encoding\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Normalize or scale numerical features using Min-Max scaling\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "scaler = MinMaxScaler()\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Save the preprocessed DataFrame\n",
    "df.to_json('preprocessed_ookla_5g_map.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4ed3e-3e74-42fd-bd71-083ee2ee84cc",
   "metadata": {},
   "source": [
    "## Feature Engineering:\r\n",
    "\r\n",
    "Create new features: Generate additional features based on domain knowledge or data analysis that might enhance the model's predictive power.\r\n",
    "Transform existing features: Perform transformations (e.g., log transformations) on features to make them more suitable for modeling.\r\n",
    "Incorporate domain knowledge: Utilize insights about the problem domain to engineer features that capture relevant information for predicting 5G deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a99e4b91-63e6-4db9-ae2e-f3e360ac5e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the preprocessed DataFrame\n",
    "df = pd.read_json('preprocessed_ookla_5g_map.json')\n",
    "\n",
    "# Create new features based on domain knowledge or data analysis\n",
    "\n",
    "# Example 1: Combine numerical features\n",
    "# If you have numerical features, replace 'numerical_feature1' and 'numerical_feature2' with actual column names\n",
    "# If you don't have numerical features, you can skip this example\n",
    "if 'numerical_feature1' in df.columns and 'numerical_feature2' in df.columns:\n",
    "    df['combined_feature'] = df['numerical_feature1'] + df['numerical_feature2']\n",
    "\n",
    "# Example 2: Extract information from categorical variables\n",
    "# If you have categorical variables, replace 'city_name' with actual column name\n",
    "# If you don't have categorical variables, you can skip this example\n",
    "if 'city_name' in df.columns:\n",
    "    def get_population(city):\n",
    "        # Replace this with your logic to get population information for a given city\n",
    "        pass\n",
    "    df['city_population'] = df['city_name'].apply(get_population)\n",
    "\n",
    "# Example 3: Interaction between numerical features\n",
    "# Similar to Example 1, replace column names with actual ones if applicable\n",
    "if 'numerical_feature1' in df.columns and 'numerical_feature2' in df.columns:\n",
    "    df['interaction_feature'] = df['numerical_feature1'] * df['numerical_feature2']\n",
    "\n",
    "# Example 4: Time-related features (if applicable)\n",
    "# Replace 'date_column' with the actual date column name if you have one\n",
    "# If you don't have time-related features, you can skip this example\n",
    "if 'date_column' in df.columns:\n",
    "    df['month'] = df['date_column'].dt.month\n",
    "    df['day_of_week'] = df['date_column'].dt.dayofweek\n",
    "\n",
    "# Example 5: Distance from major cities\n",
    "# If you have 'city_name' column, use it to calculate distances\n",
    "# Replace the function implementation with your logic to calculate distances\n",
    "if 'city_name' in df.columns:\n",
    "    def calculate_distance_from_major_cities(city):\n",
    "        # Replace this with your logic to calculate distance from major cities\n",
    "        pass\n",
    "    df['distance_from_major_city'] = df['city_name'].apply(calculate_distance_from_major_cities)\n",
    "\n",
    "# Save the DataFrame with new features\n",
    "df.to_json('feature_engineered_ookla_5g_map.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0301e8-291e-4fe9-a1d8-e2059b1da475",
   "metadata": {},
   "source": [
    "## Transform existing features:\n",
    "\n",
    "Perform transformations (log transformations) on features to make them more suitable for modeling.\n",
    "Incorporate domain knowledge: Utilize insights about the problem domain to engineer features that capture relevant information for predicting 5G deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5120cd-d9d8-46f3-9b93-cfc0b4276ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the preprocessed DataFrame\n",
    "df = pd.read_json('preprocessed_ookla_5g_map.json')\n",
    "\n",
    "# Transform existing features\n",
    "\n",
    "# Example 1: Log transformation of numerical features\n",
    "# If you have numerical features, replace 'numerical_feature' with actual column name\n",
    "# If you don't have numerical features, you can skip this example\n",
    "if 'numerical_feature' in df.columns:\n",
    "    df['log_numerical_feature'] = np.log1p(df['numerical_feature'])  # Log transformation (avoiding log(0))\n",
    "\n",
    "# Example 2: Incorporate domain knowledge\n",
    "\n",
    "# Example 2.1: Distance from urban areas\n",
    "# If you have 'city_name' column, use it to identify urban areas\n",
    "# Replace the function implementation with your logic to identify urban areas\n",
    "if 'city_name' in df.columns:\n",
    "    def is_urban_area(city):\n",
    "        # Replace this with your logic to identify urban areas\n",
    "        return True if city in ['New York', 'Los Angeles', 'Chicago'] else False\n",
    "    df['is_urban'] = df['city_name'].apply(is_urban_area)\n",
    "\n",
    "# Example 2.2: Time of day\n",
    "# If you have 'date_column' column, use it to extract time-related features\n",
    "# Replace the function implementation with your logic to extract time-related features\n",
    "if 'date_column' in df.columns:\n",
    "    def get_time_of_day(timestamp):\n",
    "        # Replace this with your logic to extract time of day\n",
    "        return 'Morning' if 6 <= timestamp.hour < 12 else 'Afternoon' if 12 <= timestamp.hour < 18 else 'Evening'\n",
    "    df['time_of_day'] = df['date_column'].apply(get_time_of_day)\n",
    "\n",
    "# Save the DataFrame with new features\n",
    "df.to_json('feature_engineered_ookla_5g_map.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df29ffab-f9b5-4e43-b35d-a16caffa7cf5",
   "metadata": {},
   "source": [
    "## Data Splitting:\r\n",
    "\r\n",
    "Split the dataset: Divide the dataset into training, validation, and test sets using an appropriate ratio (e.g., 80% training, 10% validation, 10% test).\r\n",
    "Ensure randomization: Randomly shuffle the data before splitting to prevent any biases in the dataset from affecting the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "893433f2-1dc4-47b2-b4ad-f71ed18e793c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (116733, 3) (116733,)\n",
      "Validation set shape: (14592, 3) (14592,)\n",
      "Test set shape: (14592, 3) (14592,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load the feature-engineered DataFrame\n",
    "df = pd.read_json('feature_engineered_ookla_5g_map.json')\n",
    "\n",
    "# For demonstration purposes, let's assume the target variable is 'status'.\n",
    "# Replace 'status' with the actual name of your target variable.\n",
    "target_column = 'status'\n",
    "\n",
    "# Check if the target column exists in the DataFrame\n",
    "if target_column in df.columns:\n",
    "    # Define features (X) and target variable (y)\n",
    "    X = df.drop(columns=[target_column])  # Features\n",
    "    y = df[target_column]  # Target variable\n",
    "\n",
    "    # Split the dataset into training, validation, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Print the shapes of the split datasets\n",
    "    print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "    print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "    print(\"Test set shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "    # Save the split datasets\n",
    "    X_train.to_json('X_train.json', orient='records')\n",
    "    y_train.to_json('y_train.json', orient='records')\n",
    "    X_val.to_json('X_val.json', orient='records')\n",
    "    y_val.to_json('y_val.json', orient='records')\n",
    "    X_test.to_json('X_test.json', orient='records')\n",
    "    y_test.to_json('y_test.json', orient='records')\n",
    "else:\n",
    "    print(f\"Target column '{target_column}' not found in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec8930-8e3f-423a-831b-68231c29cf62",
   "metadata": {},
   "source": [
    "The dataset has been successfully split into training, validation, and test sets with the specified ratios. Here are the shapes of the split datasets:\r\n",
    "\r\n",
    "- Training set: 116,733 samples with 3 features and corresponding target values.\r\n",
    "- Validation set: 14,592 samples with 3 features and corresponding target values.\r\n",
    "- Test set: 14,592 samples with 3 features and corresponding target values.\r\n",
    "\r\n",
    "This indicates that the data splitting process has been completed as intended, and now you can proceed with model training using the training set, model evaluation using the validation set, and final performance assessment using the tes know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35713340-0743-4048-b20b-69d14ffd44c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fbbc98-bd0e-4c5e-b823-8ed4f9e21666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4422ad12-f505-4dac-9072-57dea571091a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7953950-2ac3-454c-9b8f-45e00583af88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa557a-4f33-4fc0-a109-3e7650a9bece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3debbb88-5d8f-4ef6-985a-26040361b724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a3940-f849-4c6a-be00-18bf6259f9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a43c92-7d7f-4f0b-9fc7-56553991c09d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf93d91-0b2d-4022-a43e-21cacca7e6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
